# 2024 年 1 月 25 日

## EmoLLM

和其他学校的同学一起做的有关心理健康的大模型项目。

- 项目地址：https://github.com/aJupyter/EmoLLM
- 模型权重：https://openxlab.org.cn/models/detail/jujimeizuo/EmoLLM_Model
- 项目部署：https://openxlab.org.cn/apps/detail/jujimeizuo/EmoLLM

主要工作如下：

- 数据集：通过智谱（GLM4，我负责构造）、星火、文心一言、通义千问四个模型构造数据集，通过场景+情绪的组合并构建合适的 prompt（一定需要多轮对话，因为和心理医生交流是循序渐进的） 输入到模型中，得到数据集后经过清洗、合并，我单人贡献约 3k 条数据集。

    - prompt = f'''你是一个研究过无数具有心理健康问题的病人与心理健康医生对话的专家，请你构造一些符合实际情况的具有心理健康问题的病人和心理健康医生的连续的多轮对话记录。要求病人的问题属于{data}场景，具有{emo}情感，医生的回复尽可能包含心理辅导知识，并且能够一步步诱导病人说出自己的问题进而提供解决问题的可行方案。注意，构造的数据必须以医生的陈述为结束语，每次只需要构造一个案例并且不需要写案例一、二等等，请只返回完整的对话内容。请以如下格式返回生成的数据：
    
        病人：病人的咨询或陈述
        
        医生：医生的安抚和建议
    '''

- 模型微调：我通过 Qwen-7b-chat 为基座模型，使用上述数据集进行微调，本地部署看看效果：

![](Qwen-1.jpg)
![](Qwen-2.jpg)
...

- 添加自我认知相关数据集，不然模型就露馅了。
- 最终选用 InternLM2-7b-chat 作为基座模型，效果是最好的，也符合本次工作的要求。
- 本开源项目后续会继续维护。

## Vision Transformer

笔记链接：https://note.jujimeizuo.cn/llm/transformer/vit/

- 将 Transformer 架构应用到视觉上。
- 要将图像分割成一个个 patch，然后将 patch 作为输入，经过一系列的 Transformer Encoder，最后输出一个向量，这个向量就是图像的特征向量。这个和 NLP 对序列文本处理是一样的。（这一步也是将图像和序列文本结合起来的关键）
- 证明 ViT 不需要完全依赖 CNNs。

## LMDeploy 大模型量化部署

笔记链接：https://note.jujimeizuo.cn/llm/internlm/lec5/

- 使用 Weight Only 量化（4bit 模型）。推理时，先把 4bit 权重反量化回 FP16
- 量化是一种以参数或计算中间结果精度下降换空间节省（以及同时带来的性能提升）的策略。
    - KV Cache 量化
    - W4A16 量化



## 后续任务

- 学习深度强化学习相关内容，其中有个点是用于感知层，使用 LLM 或 VLM 来将外界的输入（比如图像、视频、语言指令）转化为向量。
    - 思考能否将点云作为 VLM 的输入，转化为向量（但是点云无序，应该不能直接输入，需要先转化为有序的序列）。
    - 思考能否将一组摄像机拍摄的图像作为 VLM 的输入，转换为向量，提取其中的特征。
    - ...
- 学习 Agent 开发，这是大模型未来发展的必然趋势。
- 学习 CUDA 编程，了解 GPU 并行相关知识，拓展知识面。