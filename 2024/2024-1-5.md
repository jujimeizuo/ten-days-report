# 2024 年 1 月 5 日

## 理论

- 学习 RWKV 语言模型，目前 Elo rating 效果比较好的模型
- `PEFT` 参数高效微调，`LoRA` 微调比较（在 `RoBERTa`、`Llama 2` 和 `Mistral 7B` 上的微调性能，其中 `RoBERTa` 的性能最好，说明不是越大越复杂的模型的性能是越好的）
- 大模型开发流程及架构

## 实践

- 用 `ChatGLM3-6B` 模型进行简单的指令微调
- 书生·浦语大模型，并用 `InternLM-Chat-7B` 模型训练智能对话、智能体工具调用、图文理解创作等等 demo。
- 学习 `LangChain` 相关组件，并封装国内外较流行的大模型，自定义 LLM，封装成本地 API


这十天大部分时间复习 `最优化` 和 `算法`，准备考试。
