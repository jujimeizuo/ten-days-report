# 2023 年 12 月 15 日

## 理论

- 分布式训练的并行策略
    - 数据并行
    - 模型并行
    - 流行并行
    - 混合并行
- 学习各种 `fine-tuning` 的方法
- 学习 `PEFT`，在低资源硬件上对大规模模型进行参数高效微调

## 实践

- 用 `Pytorch` 实现 `BERT` 模型
- 熟悉 transformers 库的 API 调用，完成整个训练过程
- 可以使用 transformers 库通过各种预训练模型（`BERT`，`RoBERTa`等）完成 `fine-tuning` 任务
- 使用 `PEFT` 库