# 2023 年 12 月 15 日

## 点云配准


### 前置知识

了解点云配准相关入门知识

笔记链接：https://note.jujimeizuo.cn/cv/pcr/prepare/

### ICP

笔记链接：https://note.jujimeizuo.cn/cv/pcr/icp/

学习点云配准最传统的方法`ICP`，了解 `ICP` 的原理，以及 `ICP` 的优化方法。

## NLP 相关

### Word2Vec

笔记链接：https://note.jujimeizuo.cn/llm/nlp/word2vec/

学习了 `Word2Vec` 的原理，以及 `Skip-Gram` 和 `CBOW` 两种模型的区别、优化方法：`Hierarchical Softmax` 和 `Negative Sampling`

### RNNs

笔记链接：https://note.jujimeizuo.cn/llm/nlp/rnns/

除了最初的 `n-gram` 模型，目前主流的 NLP 模型都是基于 `RNNs` 的，最常用的就是 `LSTM` 和 `GRU`

## Transformer 相关

### Attention

笔记链接：https://note.jujimeizuo.cn/llm/transformer/attention/

对于 `RNNs` 中存在的信息瓶颈问题，注意力机制可以很好地解决这个问题。

- `Encoder-Decoder`：一个通用的框架，**将现实问题转化为数学问题，通过求解数学问题，从而解决现实问题**
- `Seq2Seq`：输入一个序列，输出另一个序列。这种结构最重要的地方在于输入序列和输出序列的长度可以可变的。具体的使用方法都属于`Encoder-Decoder` 模型的范畴
- `Attention`：核心思想是 **带权求和**。将 `attention` 引入 `Encoder-Decoder` 框架效果更好，但也可以脱离 `Encoder-Decoder`。

### transformer

笔记链接：https://note.jujimeizuo.cn/llm/transformer/transformer/#_1

transformer 总体为一个编码组件、解码组件以及二者之间连接。

相比于 RNN，巨大的优点是 **模型在处理序列输入时，可以对整个序列输入进行并行计算，不需要按照时间步循环递归处理输入序列。**


## LLM

### Prompt

笔记链接：https://note.jujimeizuo.cn/llm/prompt/

针对开发者在 ChatGPT 的基础上学习 prompt 的提示词艺术，包括准则和一些技巧。